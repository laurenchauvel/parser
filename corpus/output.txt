A Scalable MMR Approach to Sentence Scoring
  for Multi-Document Update Summarization

Florian Boudin and Marc El-Be`ze                               Juan-Manuel Torres-Moreno ,
  Laboratoire Informatique d'Avignon                            E´ cole Polytechnique de Montre´al
                                                             CP 6079 Succ. Centre Ville H3C 3A7
 339 chemin des Meinajaries, BP1228,
    84911 Avignon Cedex 9, France.                                Montre´al (Que´bec), Canada.

    florian.boudin@univ-avignon.fr                           juan-manuel.torres@univ-avignon.fr

       marc.elbeze@univ-avignon.fr

                      Abstract                               redundancy with previously read documents (his-
                                                             tory) has to be removed from the extract.
    We present SMMR, a scalable sentence
    scoring method for query-oriented up-                       A natural way to go about update summarization
    date summarization. Sentences are scored                 would be extracting temporal tags (dates, elapsed
    thanks to a criterion combining query rele-              times, temporal expressions...) (Mani and Wilson,
    vance and dissimilarity with already read                2000) or to automatically construct the timeline
    documents (history). As the amount of                    from documents (Swan and Allan, 2000). These
    data in history increases, non-redundancy                temporal marks could be used to focus extracts on
    is prioritized over query-relevance. We                  the most recently written facts. However, most re-
    show that SMMR achieves promising re-                    cently written facts are not necessarily new facts.
    sults on the DUC 2007 update corpus.                     Machine Reading (MR) was used by (Hickl et
                                                             al., 2007) to construct knowledge representations
1 Introduction                                               from clusters of documents. Sentences contain-
                                                             ing "new" information (i.e. that could not be in-
Extensive experiments on query-oriented multi-               ferred by any previously considered document)
document summarization have been carried out                 are selected to generate summary. However, this
over the past few years. Most of the strategies              highly efficient approach (best system in DUC
to produce summaries are based on an extrac-                 2007 update) requires large linguistic resources.
tion method, which identifies salient textual seg-           (Witte et al., 2007) propose a rule-based system
ments, most often sentences, in documents. Sen-              based on fuzzy coreference cluster graphs. Again,
tences containing the most salient concepts are se-          this approach requires to manually write the sen-
lected, ordered and assembled according to their             tence ranking scheme. Several strategies remain-
relevance to produce summaries (also called ex-              ing on post-processing redundancy removal tech-
tracts) (Mani and Maybury, 1999).                            niques have been suggested. Extracts constructed
                                                             from history were used by (Boudin and Torres-
   Recently emerged from the Document Under-                 Moreno, 2007) to minimize history's redundancy.
standing Conference (DUC) 20071, update sum-                 (Lin et al., 2007) have proposed a modified Max-
marization attempts to enhance summarization                 imal Marginal Relevance (MMR) (Carbonell and
when more information about knowledge acquired               Goldstein, 1998) re-ranker during sentence selec-
by the user is available. It asks the following ques-        tion, constructing the summary by incrementally
tion: has the user already read documents on the             re-ranking sentences.
topic? In the case of a positive answer, producing
an extract focusing on only new facts is of inter-              In this paper, we propose a scalable sentence
est. In this way, an important issue is introduced:          scoring method for update summarization derived
                                                             from MMR. Motivated by the need for relevant
      c 2008. Licensed under the Creative Commons            novelty, candidate sentences are selected accord-
Attribution-Noncommercial-Share Alike 3.0 Unported li-       ing to a combined criterion of query relevance and
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).   dissimilarity with previously read sentences. The
Some rights reserved.                                        rest of the paper is organized as follows. Section 2

    1Document Understanding Conferences are conducted
since 2000 by the National Institute of Standards and Tech-
nology (NIST), http://www-nlpir.nist.gov

                                    23

Coling 2008: Companion volume ­ Posters and Demonstrations, pages 23­26
                               Manchester, August 2008
introduces our proposed sentence scoring method            sentence s and the query Q:
and Section 3 presents experiments and evaluates
our approach.                                              JWe(s, Q)  =   1   ·       max  JW(q, m)       (1)
                                                                         |Q|
2 Method                                                                         qQ mS

The underlying idea of our method is that as the           where S is the term set of s in which the terms
number of sentences in the history increases, the          m that already have maximized JW(q, m) are re-
likelihood to have redundant information within            moved. The use of JWe smooths normalization and
candidate sentences also increases. We propose             misspelling errors. Each sentence s is scored using
a scalable sentence scoring method derived from            the linear combination:
MMR that, as the size of the history increases,
gives more importance to non-redundancy that to               Sim1(s, Q) =  · cosine(s, Q)
query relevance. We define H to represent the pre-                                   + (1 - ) · JWe(s, Q) (2)
viously read documents (history), Q to represent
the query and s the candidate sentence. The fol-           where  = 0.7, optimally tuned on the past DUCs
lowing subsections formally define the similarity          data (2005 and 2006). The system produces a list
measures and the scalable MMR scoring method.              of ranked sentences from which the summary is
                                                           constructed by arranging the high scored sentences
2.1 A query-oriented multi-document                        until the desired size is reached.
      summarizer
                                                           2.2 A scalable MMR approach
We have first started by implementing a simple
summarizer for which the task is to produce query-         MMR re-ranking algorithm has been successfully
focused summaries from clusters of documents.              used in query-oriented summarization (Ye et al.,
Each document is pre-processed: documents are              2005). It strives to reduce redundancy while main-
segmented into sentences, sentences are filtered           taining query relevance in selected sentences. The
(words which do not carry meaning are removed              summary is constructed incrementally from a list
such as functional words or common words) and              of ranked sentences, at each iteration the sentence
normalized using a lemmas database (i.e. inflected         which maximizes MMR is chosen:
forms "go", "goes", "went", "gone"... are replaced
by "go"). An N -dimensional term-space  , where               MMR = arg max [  · Sim1(s, Q)
N is the number of different terms found in the
cluster, is constructed. Sentences are represented                                  sS
in  by vectors in which each component is the
term frequency within the sentence. Sentence scor-                         - (1 - ) · max Sim2(s, sj) ] (3)
ing can be seen as a passage retrieval task in Infor-
mation Retrieval (IR). Each sentence s is scored by                                                  sj E
computing a combination of two similarity mea-
sures between the sentence and the query. The first        where S is the set of candidates sentences and E
measure is the well known cosine angle (Salton et          is the set of selected sentences.  represents an
al., 1975) between the sentence and the query vec-         interpolation coefficient between sentence's rele-
torial representations in  (denoted respectively s         vance and non-redundancy. Sim2(s, sj) is a nor-
and Q). The second similarity measure is based             malized Longest Common Substring (LCS) mea-
on the Jaro-Winkler distance (Winkler, 1999). The          sure between sentences s and sj. Detecting sen-
original Jaro-Winkler measure, denoted JW, uses            tence rehearsals, LCS is well adapted for redun-
the number of matching characters and transposi-           dancy removal.
tions to compute a similarity score between two
terms, giving more favourable ratings to terms that           We propose an interpretation of MMR to tackle
match from the beginning. We have extended this            the update summarization issue. Since Sim1 and
measure to calculate the similarity between the            Sim2 are ranged in [0, 1], they can be seen as prob-
                                                           abilities even though they are not. Just as rewriting
                                                           (3) as (NR stands for Novelty Relevance):

                                                           NR = arg max [  · Sim1(s, Q)

                                                                          sS

                                                           + (1 - )   ·  (1  -   max    Sim2(s,  sh))  ]  (4)

                                                                                 shH

                                                           We can understand that (4) equates to an OR com-
                                                           bination. But as we are looking for a more intu-
                                                           itive AND and since the similarities are indepen-
                                                           dent, we have to use the product combination. The

                                                       24
scoring method defined in (2) is modified into a               3. An update summary of documents in C, un-
double maximization criterion in which the best                   der the assumption that the reader has already
ranked sentence will be the most relevant to the                  read documents in A and B.
query AND the most different to the sentences in
H.                                                             Within a topic, the document clusters must be pro-
                                                               cessed in chronological order. Our system gener-
SMMR(s) = Sim1(s, Q)                                           ates a summary for each cluster by arranging the
                                                               high ranked sentences until the limit of 100 words
                                        f (H)                  is reached.

·  1  -  max          S  im2  (s,  sh)         (5)

         shH

Decreasing  in (3) with the length of the sum-                 3.2 Evaluation
mary was suggested by (Murray et al., 2005) and
successfully used in the DUC 2005 by (Hachey                   Most existing automated evaluation methods work
et al., 2005), thereby emphasizing the relevance               by comparing the generated summaries to one or
at the outset but increasingly prioritizing redun-             more reference summaries (ideally, produced by
dancy removal as the process continues. Sim-                   humans). To evaluate the quality of our generated
ilarly, we propose to follow this assumption in                summaries, we choose to use the ROUGE3 (Lin,
SMMR using a function denoted f that as the                    2004) evaluation toolkit, that has been found to be
amount of data in history increases, prioritize non-           highly correlated with human judgments. ROUGE-
redundancy (f (H)  0).                                         N is a n-gram recall measure calculated between
                                                               a candidate summary and a set of reference sum-
3 Experiments                                                  maries. In our experiments ROUGE-1, ROUGE-2
                                                               and ROUGE-SU4 will be computed.
The method described in the previous section has
been implemented and evaluated by using the                    3.3 Results
DUC 2007 update corpus2. The following subsec-
tions present details of the different experiments             Table 1 reports the results obtained on the DUC
we have conducted.
                                                               2007 update data set for different sentence scor-

                                                               ing methods. cosine + JWe stands for the scor-

                                                               ing method defined in (2) and NR improves it

3.1 The DUC 2007 update corpus                                 with sentence re-ranking defined in equation (4).

We used for our experiments the DUC 2007 up-                   SMMR is the combined adaptation we have pro-
date competition data set. The corpus is composed
of 10 topics, with 25 documents per topic. The up-             posed in (5). The function f (H) used in SMMR is
date task goal was to produce short (100 words)
multi-document update summaries of newswire ar-                the  simple   rational  function   1  ,  where  H  increases
ticles under the assumption that the user has al-                                                 H
ready read a set of earlier articles. The purpose              with the number of previous clusters (f (H) = 1
of each update summary will be to inform the
reader of new information about a particular topic.            for  cluster  A,  1  for  cluster  B  and  1  for  cluster  C).
Given a DUC topic and its 3 document clusters: A                                 2                        3
(10 documents), B (8 documents) and C (7 doc-                  This function allows to simply test the assumption
uments), the task is to create from the documents
three brief, fluent summaries that contribute to sat-          that non-redundancy have to be favoured as the
isfying the information need expressed in the topic
statement.                                                     size of history grows. Baseline results are obtained

                                                               on summaries generated by taking the leading sen-

                                                               tences of the most recent documents of the cluster,

                                                               up to 100 words (official baseline of DUC). The

                                                               table also lists the three top performing systems at

                                                               DUC 2007 and the lowest scored human reference.

                                                               As we can see from these results, SMMR out-

                                                               performs the other sentence scoring methods. By

                                                               ways of comparison our system would have been

1. A summary of documents in cluster A.                        ranked second at the DUC 2007 update competi-

                                                               tion. Moreover, no post-processing was applied to

  2. An update summary of documents in B, un-                  the selected sentences leaving an important margin
      der the assumption that the reader has already
      read documents in A.                                     of progress. Another interesting result is the high

    2More information about the DUC 2007 corpus is avail-      performance of the non-update specific method
able at http://duc.nist.gov/.
                                                               (cosine + JWe) that could be due to the small size

                                                               3ROUGE is available at http://haydn.isi.edu/ROUGE/.

                                                           25
of the corpus (little redundancy between clusters).        Hachey, B., G. Murray, and D. Reitter. 2005. The
                                                              Embra System at DUC 2005: Query-oriented Multi-
Baseline      ROUGE-1  ROUGE-2  ROUGE-SU4                     document Summarization with a Very Large Latent
3rd system                                                    Semantic Space. In Document Understanding Con-
2nd system    0.26232  0.04543   0.08247                      ference (DUC).
              0.35715  0.09622   0.13245
cosine + JWe  0.36965  0.09851   0.13509                   Hickl, A., K. Roberts, and F. Lacatusu. 2007. LCC's
              0.35905  0.10161   0.13701                      GISTexter at DUC 2007: Machine Reading for Up-
NR            0.36207  0.10042   0.13781                      date Summarization. In Document Understanding
              0.36323  0.10223   0.13886                      Conference (DUC).
SMMR          0.37032  0.11189   0.14306
1st system    0.40497  0.10511   0.14779                   Lin, Z., T.S. Chua, M.Y. Kan, W.S. Lee, L. Qiu, and
Worst human                                                   S. Ye. 2007. NUS at DUC 2007: Using Evolu-
                                                              tionary Models of Text. In Document Understanding
                                                              Conference (DUC).

Table 1: ROUGE average recall scores computed              Lin, C.Y. 2004. Rouge: A Package for Automatic
                                                              Evaluation of Summaries. In Workshop on Text Sum-
on the DUC 2007 update corpus.
                                                           marization Branches Out, pages 25­26.

4 Discussion and Future Work                               Mani, I. and M.T. Maybury. 1999. Advances in Auto-
                                                              matic Text Summarization. MIT Press.
In this paper we have described SMMR, a scal-
able sentence scoring method based on MMR that             Mani, I. and G. Wilson. 2000. Robust temporal pro-
achieves very promising results. An important as-             cessing of news. In 38th Annual Meeting on Asso-
pect of our sentence scoring method is that it does           ciation for Computational Linguistics, pages 69­76.
not requires re-ranking nor linguistic knowledge,             Association for Computational Linguistics Morris-
which makes it a simple and fast approach to the              town, NJ, USA.
issue of update summarization. It was pointed out
at the DUC 2007 workshop that Question Answer-             Murray, G., S. Renals, and J. Carletta. 2005. Extractive
ing and query-oriented summarization have been                Summarization of Meeting Recordings. In Ninth Eu-
converging on a common task. The value added                  ropean Conference on Speech Communication and
by summarization lies in the linguistic quality. Ap-          Technology. ISCA.
proaches mixing IR techniques are well suited for
query-oriented summarization but they require in-          Salton, G., A. Wong, and C. S. Yang. 1975. A vector
tensive work for making the summary fluent and                space model for automatic indexing. Communica-
coherent. Among the others, this is a point that we           tions of the ACM, 18(11):613­620.
think is worthy of further investigation.
                                                           Swan, R. and J. Allan. 2000. Automatic generation
Acknowledgments                                               of overview timelines. In 23rd annual international
                                                              ACM SIGIR conference on Research and develop-
This work was supported by the Agence Nationale               ment in information retrieval, pages 49­56.
de la Recherche, France, project RPM2.
                                                           Winkler, W. E. 1999. The state of record linkage and
                                                              current research problems. In Survey Methods Sec-
                                                              tion, pages 73­79.

                                                           Witte, R., R. Krestel, and S. Bergler. 2007. Generat-
                                                              ing Update Summaries for DUC 2007. In Document
                                                              Understanding Conference (DUC).

References                                                 Ye, S., L. Qiu, T.S. Chua, and M.Y. Kan. 2005. NUS
                                                              at DUC 2005: Understanding documents via con-
Boudin, F. and J.M. Torres-Moreno. 2007. A Co-                cept links. In Document Understanding Conference
   sine Maximization-Minimization approach for User-          (DUC).
   Oriented Multi-Document Update Summarization.
   In Recent Advances in Natural Language Processing
   (RANLP), pages 81­87.

Carbonell, J. and J. Goldstein. 1998. The use of MMR,
   diversity-based reranking for reordering documents
   and producing summaries. In 21st annual interna-
   tional ACM SIGIR conference on Research and de-
   velopment in information retrieval, pages 335­336.
   ACM Press New York, NY, USA.

                                                       26
