Extraction of terminology in the ﬁeld of
construction

1st R´emy Kessler
Universit´e Bretagne Sud
CNRS 6074A
56017 Vannes,France
remy.kessler@univ-ubs.fr

2nd Nicolas B´echet
Universit´e Bretagne Sud
CNRS 6074A
56017 Vannes,France
nicolas.bechet@irisa.fr

3rd Giuseppe Berio
Universit´e Bretagne Sud
CNRS 6074A
56017 Vannes,France
giuseppe.berio@univ-ubs.fr

Abstract—We describe a corpus analysis method to extract
terminology from a collection of technical speciﬁcations in the
ﬁeld of construction. Using statistics and word n-grams analysis,
we extract the terminology of the domain and then perform
pruning steps with linguistic patterns and internet queries to
improve the quality of the ﬁnal terminology. Results are evaluated
by using a manual evaluation carried out by 6 experts in the ﬁeld.

Index Terms—terminology extraction, Internet queries, linguis-

tic patterns.

I. INTRODUCTION

The current era is increasingly inﬂuenced by the prominence
of smart data and mobile applications. The work presented
in this paper has been carried out in one industrial project
(VOCAGEN) aiming at automating the production of struc-
tured data from human machine dialogues. Speciﬁcally, the
targeted application drives dialogues with people working in
a construction area for populating a database reporting key
data extracted from those dialogues. This application requires
complex processing for both transcripting speeches but also for
driving dialogues. The ﬁrst process is required for good speech
recognition in a noisy environment. The second processing is
required because the database needs to be populated with both
right and complete data; indeed, people tend to apply a broad
(colloquial) vocabulary and the transcripted words need to be
used for ﬁlling in the corresponding data. Additionally, if some
data populate the database, additional data may be required
for completeness, thus the dialogue should enable to get those
additional data (e.g. if the word ”room” is recognised and used
to populate the database, the location of the room must also
be got; this can be done by driving the dialogue).

The application provides people with “hand-free” device,
enabling a complete, quick and standardized reporting. First
usages of this application will be oriented to reporting failures
and problems in constructions.

The two processing steps mentioned above require on the
one side a “language model” (for transcripting the sentences)
and on the other side a “knowledge model” for driving the

dialogue and correctly understanding the meaning of the word.
The knowledge model is mainly an ontology of the domain (in
this case, the construction domain) providing the standardized
concepts and their relationships. As well-known, building such
knowledge models needs time and is costly; one of the earlier
questions raised by our industrial partners has been about
“how to build, as automaticaly as possible, such a knowledge
model”. This question is closely related to the interest of
quickly adapting the application to other domains (than the
construction one) for reaching new markets. We developed a
complete methodology and system for partially answering the
question, focusing on how to extract a relevant terminology
from a collection of technical speciﬁcations.

The rest of the paper is organized as follow. Section II
present context of the project. Related work are reviewed in
Section III. Section IV presents collected resources and some
statistics about them. Section V describes the methodology de-
veloped for extracting relevant terms from collected resources.
The details about the evaluation are presented in Section VI-A
and results obtained, are given in Section VI-B.

II. INDUSTRIAL CONTEXT

Figure 1 presents the context of this work in VOCA-
GEN project. Our industrial partner Script&Go1 develop an
application for the construction management dedicated to
touch devices and wishes to set up an oral dialogue module
to facilitate on construction site seizure. The second industrial
partner (Tykomz) develops a vocal recognition suite based on
toolkit sphynx 4 [1]. This toolkit includes hierarchical agglom-
erative clustering methods using well-known measures such as
BIC and CLR and provides elementary tools, such as segment
and cluster generators, decoder and model trainers. Fitting
those elementary tools together is an easy way of developing
a speciﬁc diarization system. To work, it is necessary to build
a model of knowledge, i.e. a model describing the expressions
that must be recognized by the program. To improve the
performance of the system, this knowledge model must be

1http://www.scriptandgo.com/en/

ﬁeld of nanotechnology. [14] propose TAXI, which combines
statistics and learning approach. TAXI is a system for building
a taxonomy using 2 corpora, a generic, the other speciﬁc.
It ranks the relevance of candidates by measure (frequency-
based), and by learning with SVM. [15], [16] present TexSIS,
a bilingual terminology extraction system with chunk-based
alignment method for the generation of candidate terms. After
the corpus alignment step, they use an approach combining
log likelihood measure and Mutual Expectation measure [17]
to rank candidate terms in each language. In the same or-
der, [18], [19] present an approach to extract grammatical
terminology from linguistic corpora. They compare a series
of well-established statistical measures that have been used
in similar automatic term extraction tasks and conclude that
corpus-comparing methods perform better than metrics that
are not based on corpus comparison. [20] and [21] present
methods with word embeddings. With a small data set for
learning phase, they improve the term extraction results in
n-gram terms. However, these papers involve labelled data
sets for learning phase, which is the main difference with
our proposed approach. The originality of our approach is to
combine a lexico-syntactical and a statistical approach while
using external resources.

IV. RESOURCES AND STATISTICS
First experiments were carried out using technical reports
collected from some customers from our industrial partners
who will be called as NC collection thereafter. Each document
contains all the non-compliance that was found on one work
site and describe solutions to resolve it. However heterogeneity
of the formats as well as the artiﬁcial repetition of the informa-
tion between two reports found in the same construction site
made the term extraction quite difﬁcult. An insightful analysis
of those reports reveals vocabulary richness, however, difﬁcult
to exploit given numerous misspellings, typing shortcuts, very
”telegraphic” style with verbs in the inﬁnitive, little punctu-
ation, few determinants, etc. As a consequence, we used a
collection of technical speciﬁcations called CCTP2. CCTPs are
available online on public sector websites3. Several thousand
documents were collected by our industrial partner using an
automatic web collecting process. Figure 2 presents some key
descriptive statistics of theses collections.

Collection
Total number of documents

NC
58 402

CCTP

3665

Without pre-processing

Total number of words
Total number of different words
Average words/document

130 309
93 000
125.3

230 962 734
20 6264
63 018.48

Fig. 2. statistics of the collection.

2The technical speciﬁcations book (CCTP in French) is a contractual
document that gathers the technical clauses of a public contract in the ﬁeld
of construction.

3For

example,
http://marchespublics.normandie.fr/.

https://www.marches-publics.gouv.fr/

or

Fig. 1. ﬁgure describing the context of the project

powered by a domain-speciﬁc vocabulary. For example, in the
sentence ”there is a stain of paint in the kitchen”, the system
must understand that it is a stain of paint and that the kitchen is
a room. To our knowledge, there is no ontology or taxonomy
speciﬁc to the construction industry in French. A version is
under development by [2] but ontology is in English and very
generic. We therefore choose to extract useful knowledge from
textual data, and then, in a second step, to organize them.

III. RELATED WORKS

The goal of ontology learning (OL) is to build knowledge
models from text. OL use NLP knowledge extraction tools
to extract
terminology (concepts) and links between them
(relationships). The main approaches found in the literature are
rule-based systems, statistical or learning based approaches.

The reference in the ﬁeld of rule-based systems was de-
veloped by [3]. General Architecture for Text Engineering
(GATE) is a Java collection of tools initially developed at the
University of Shefﬁeld since 1995. An alternative is offered by
the existing semi-automatic ontology learning text2onto [4].
More recently, [5] developed UIMA, a system that can be
positioned as an alternative to GATE. Amongst other things,
UIMA makes possible to generate rules from a collection of
annotated documents. Exit, introduced by [6] is an iterative
approach that ﬁnds the terms in an incremental way.

[7] with TERMINAE is certainly the oldest statistic ap-
proach. Developed for French and based on lexical frequen-
cies, it requires pre-processing with TermoStat [8] and ANNIE
(GATE). [9] presents a method for extracting terminology spe-
ciﬁc to a domain from a corpus of domain-speciﬁc text, where
no external general domain reference corpus is required. They
present an adaptation of the classic tf-idf as ranking measure
and use different ﬁlters to produce a speciﬁc terminology.
More recently, the efﬁciency of ranking measure like mutual
information developed for statistical approach is discussed in
[10] and [11]. [12] proposes Termolator a terminology extrac-
tion system using a chunking procedure, and using internet
queries for ranking candidate terms. Approach is interesting
but the authors emphasize the fact that the runtime for each
queries is a limiting factor to produce a relevant ranking.

Closer to our work, [13] presents an approach combining
linguistic pattern and Z-score to extract terminology in the

  