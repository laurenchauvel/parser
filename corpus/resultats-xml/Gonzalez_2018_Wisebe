<article>
	<titre>
WiSeBE: Window-Based Sentence Boundary Evaluation </titre>
	<auteurs>
	</auteurs>
	<abstract>
Abstract. Sentence Boundary Detection (SBD) has been a major
research topic since Automatic Speech Recognition transcripts have been
used for further Natural Language Processing tasks like Part of Speech
Tagging, Question Answering or Automatic Summarization. But what
about evaluation? Do standard evaluation metrics like precision, recall,
F-score or classiﬁcation error; and more important, evaluating an auto-
matic system against a unique reference is enough to conclude how well
a SBD system is performing given the ﬁnal application of the transcript?
In this paper we propose Window-based Sentence Boundary Evaluation
(WiSeBE), a semi-supervised metric for evaluating Sentence Boundary
Detection systems based on multi-reference (dis)agreement. We evalu-
ate and compare the performance of diﬀerent SBD systems over a set
of Youtube transcripts using WiSeBE and standard metrics. This dou-
ble evaluation gives an understanding of how WiSeBE is a more reliable
metric for the SBD task. </abstract>
	<introduction>
 Introduction The goal of Automatic Speech Recognition (ASR) is to transform spoken data into a written representation, thus enabling natural human-machine interaction [ 33 ] with further Natural Language Processing (NLP) tasks. Machine transla- tion, question answering, semantic parsing, POS tagging, sentiment analysis and automatic text summarization; originally developed to work with formal writ- ten texts, can be applied over the transcripts made by ASR systems [ 2 , 25 , 31 ]. However, before applying any of these NLP tasks a segmentation process called Sentence Boundary Detection (SBD) should be performed over ASR transcripts to reach a minimal syntactic information in the text. To measure the performance of a SBD system, the automatically segmented transcript is evaluated against a single reference normally done by a human. But c ⃝  Springer Nature Switzerland AG 2018 I. Batyrshin et al. (Eds.): MICAI 2018, LNAI 11289, pp. 119–131, 2018. https://doi.org/10.1007/978-3-030-04497-8 _ 10 120 C.-E. Gonz´alez-Gallardo and J.-M. Torres-Moreno given a transcript, does it exist a unique reference? Or, is it possible that the same transcript could be segmented in ﬁve diﬀerent ways by ﬁve diﬀerent people in the same conditions? If so, which one is correct; and more important, how to fairly evaluate the automatically segmented transcript? These questions are the foundations of Window-based Sentence Boundary Evaluation (WiSeBE), a new semi-supervised metric for evaluating SBD systems based on multi-reference (dis)agreement. The rest of this article is organized as follows. In Sect.  2  we set the frame of SBD and how it is normally evaluated. WiSeBE is formally described in Sect.  3 , followed by a multi-reference evaluation in Sect.  4 . Further analysis of WiSeBE and discussion over the method and alternative multi-reference evaluation is presented in Sect.  5 . Finally, Sect.  6  concludes the paper. </introduction>
	<corps>
 2 Sentence Boundary Detection Sentence Boundary Detection (SBD) has been a major research topic science ASR moved to more general domains as conversational speech [ 17 , 24 , 26 ]. Per- formance of ASR systems has improved over the years with the inclusion and combination of new Deep Neural Networks methods [ 5 , 9 , 33 ]. As a general rule, the output of ASR systems lacks of any syntactic information such as capital- ization and sentence boundaries, showing the interest of ASR systems to obtain the correct sequence of words with almost no concern of the overall structure of the document [ 8 ]. Similar to SBD is the Punctuation Marks Disambiguation (PMD) or Sentence Boundary Disambiguation. This task aims to segment a formal written text into well formed sentences based on the existent punctuation marks [ 11 , 19 , 20 , 29 ]. In this context a sentence is deﬁned (for English) by the Cambridge Dictionary 1 as: “a group of words, usually containing a verb, that expresses a thought in the form of a statement, question, instruction, or exclamation and starts with a capital letter when written” . PMD carries certain complications, some given the ambiguity of punctuation marks within a sentence. A period can denote an acronym, an abbreviation, the end of the sentence or a combination of them as in the following example: The U.S. president, Mr. Donald Trump, is meeting with the F.B.I. director Christopher A. Wray next Thursday at 8 p.m. However its diﬃculties, DPM proﬁts of morphological and lexical information to achieve a correct sentence segmentation. By contrast, segmenting an ASR transcript should be done without any (or almost any) lexical information and a ﬂurry deﬁnition of sentence. 1  https://dictionary.cambridge.org/ . WiSeBE: Window-Based Sentence Boundary Evaluation 121 The obvious division in spoken language may be considered speaker utter- ances. However, in a normal conversation or even in a monologue, the way ideas are organized diﬀers largely from written text. This diﬀerences, added to disﬂu- encies like revisions, repetitions, restarts, interruptions and hesitations make the deﬁnition of a sentence unclear thus complicating the segmentation task [ 27 ]. Table  1  exempliﬁes some of the diﬃculties that are present when working with spoken language. Table 1.  Sentence Boundary Detection example Speech transcript SBD applied to transcript two two women can look out after a kid so bad as a man and a woman can so you can have a you can have a mother and a father that that still don’t do right with the kid and you can have to men that can so as long as the love each other as long as they love each other it doesn’t matter two // two women can look out after a kid so bad as a man and a woman can // so you can have a // you can have a mother and a father that // that still don’t do right with the kid and you can have to men that can // so as long as the love each other // as long as they love each other it doesn’t matter// Stolcke and Shriberg [ 26 ] considered a set of linguistic structures as segments including the following list: – Complete sentences – Stand-alone sentences – Disﬂuent sentences aborted in mid-utterance – Interjections – Back-channel responses. In [ 17 ], Meteer and Iyer divided speaker utterances into segments, consisting each of a single independent clause. A segment was considered to begin either at the beginning of an utterance, or after the end of the preceding segment. Any dysﬂuency between the end of the previous segments and the begging of current one was considered part of the current segments. Rott and  ˇ Cerva [ 23 ] aimed to summarize news delivered orally segmenting the transcripts into  “something that is similar to sentences” . They used a syntactic analyzer to identify the phrases within the text. A wide study focused in unbalanced data for the SBD task was performed by Liu  et al.  [ 15 ]. During this study they followed the segmentation scheme pro- posed by the Linguistic Data Consortium 2   on the Simple Metadata Annotation Speciﬁcation V5.0 guideline (SimpleMDE V5.0) [ 27 ], dividing the transcripts in Semantic Units. 2  https://www.ldc.upenn.edu/ . 122 C.-E. Gonz´alez-Gallardo and J.-M. Torres-Moreno A Semantic Unit (SU) is considered to be an atomic element of the transcript that manages to express a complete thought or idea on the part of the speaker [ 27 ]. Sometimes a SU corresponds to the equivalent of a sentence in written text, but other times (the most part of them) a SU corresponds to a phrase or a single word. SUs seem to be an inclusive conception of a segment, they embrace diﬀerent previous segment deﬁnitions and are ﬂexible enough to deal with the majority of spoken language troubles. For these reasons we will adopt SUs as our segment deﬁnition. 2.1 Sentence Boundary Evaluation SBD research has been focused on two diﬀerent aspects; features and methods. Regarding the features, some work focused on acoustic elements like pauses duration, fundamental frequencies, energy, rate of speech, volume change and speaker turn [ 10 , 12 , 14 ]. The other kind of features used in SBD are textual or lexical features. They rely on the transcript content to extract features like bag-of-word, POS tags or word embeddings [ 7 , 12 , 16 , 18 , 23 , 26 , 30 ]. Mixture of acoustic and lexical features have also been explored [ 1 , 13 , 14 , 32 ], which is advantageous when both audio signal and transcript are available. With respect to the methods used for SBD, they mostly rely on statisti- cal/neural machine translation [ 12 , 22 ], language models [ 8 , 15 , 18 , 26 ], conditional random ﬁelds [ 16 , 30 ] and deep neural networks [ 3 , 7 , 29 ]. Despite their diﬀerences in features and/or methodology, almost all previous cited research share a common element; the evaluation methodology. Metrics as Precision, Recall, F1-score, Classiﬁcation Error Rate and Slot Error Rate (SER) are used to evaluate the proposed system against one reference. As discussed in Sect.  1 , further NLP tasks rely on the result of SBD, meaning that is crucial to have a good segmentation. But comparing the output of a system against a unique reference will provide a reliable score to decide if the system is good or bad? Bohac  et al.  [ 1 ] compared the human ability to punctuate recognized spon- taneous speech. They asked 10 people (correctors) to punctuate about 30 min of ASR transcripts in Czech. For an average of 3,962 words, the punctuation marks placed by correctors varied between 557 and 801; this means a diﬀerence of 244 segments for the same transcript. Over all correctors, the absolute consensus for period (.) was only 4.6% caused by the replacement of other punctuation marks as semicolons (;) and exclamation marks (!). These results are understandable if we consider the diﬃculties presented previously in this section. To our knowledge, the amount of studies that have tried to target the sentence boundary evaluation with a multi-reference approach is very small. In [ 1 ], Bohac et al.  evaluated the overall punctuation accuracy for Czech in a straightforward multi-reference framework. They considered a period (.) valid if at least ﬁve of their 10 correctors agreed on its position. WiSeBE: Window-Based Sentence Boundary Evaluation 123 Kol´aˇr and Lamel [ 13 ] considered two independent references to evaluate their system and proposed two approaches. The ﬁst one was to calculate the SER for each of one the two available references and then compute their mean. They found this approach to be very strict because for those boundaries where no agreement between references existed, the system was going to be partially wrong even the fact that it has correctly predicted the boundary. Their second app- roach tried to moderate the number of unjust penalizations. For this case, a classiﬁcation was considered incorrect only if it didn’t match either of the two references. These two examples exemplify the real need and some straightforward solu- tions for multi-reference evaluation metrics. However, we think that it is possible to consider in a more inclusive approach the similarities and diﬀerences that mul- tiple references could provide into a sentence boundary evaluation protocol. 3 Window-Based Sentence Boundary Evaluation Window-Based Sentence Boundary Evaluation (WiSeBE) is a semi-automatic multi-reference sentence boundary evaluation protocol which considers the per- formance of a candidate segmentation over a set of segmentation references and the agreement between those references. Let  R  =  { R 1 , R 2 , ..., R m }  be the set of all available references given a tran- script  T  =  { t 1 , t 2 , ..., t n } , where  t j  is the  j th   word in the transcript; a reference R i  is deﬁned as a binary vector in terms of the existent SU boundaries in  T . R i  =  { b 1 , b 2 , ..., b n } (1) where b j  =  1 if  t j  is a boundary 0 otherwise Given a transcript  T , the candidate segmentation  C T  is deﬁned similar to  R i . C T  =  { b 1 , b 2 , ..., b n } (2) where b j  =  1 if  t j  is a boundary 0 otherwise 3.1 General Reference and Agreement Ratio A General Reference ( R G ) is then constructed to calculate the agreement ratio between all references in. It is deﬁned by the boundary frequencies of each ref- erence  R i  ∈  R . R G  =  { d 1 , d 2 , ..., d n } (3) where 124 C.-E. Gonz´alez-Gallardo and J.-M. Torres-Moreno d j  = m  i =1 t ij ∀ t j  ∈  T, d j  = [0 , m ] (4) The Agreement Ratio ( R G AR ) is needed to get a numerical value of the dis- tribution of SU boundaries over  R . A value of  R G AR  close to 0 means a low agreement between references in  R , while  R G AR  = 1 means a perfect agreement ( ∀ R i  ∈  R , R i  =  R i +1 | i  = 1 , ..., m  −  1) in  R . R G AR  =   R G P B R G HA (5) In the equation above,  R G P B  corresponds to the ponderated common boundaries of  R G  and  R G HA  to its hypothetical maximum agreement. R G P B  = n  j =1 d j  [ d j  ≥  2] (6) R G HA  =  m  ×  d j ∈ R G 1 [ d j  ̸ = 0] (7) 3.2 Window-Boundaries Reference In Sect.  2  we discussed about how disﬂuencies complicate SU segmentation. In a multi-reference environment this causes disagreement between references around a same SU boundary. The way WiSeBE handle disagreements produced by dis- ﬂuencies is with a Window-boundaries Reference ( R W  ) deﬁned as: R W  =  { w 1 , w 2 , ..., w p } (8) where each window  w k  considers one or more boundaries  d j  from  R G  with a window separation limit equal to  R W l . w k  =  { d j , d j +1 , d j +2 , ... } (9) 3.3 W iSeBE WiSeBE  is a normalized score dependent of (1) the performance of  C T  over  R W and (2) the agreement between all references in  R . It is deﬁned as: WiSeBE  =  F 1 R W  ×  R G AR WiSeBE  = [0 ,  1] (10) where  F 1 R W  corresponds to the harmonic mean of precision and recall of  C T with respect to  R W  (Eq.  11 ), while  R G AR  is the agreement ratio deﬁned in ( 5 ). R G AR  can be interpreted as a scaling factor; a low value will penalize the overall WiSeBE  score given the low agreement between references. By contrast, for a high agreement in  R  ( R G AR  ≈  1),  WiSeBE  ≈  F 1 R W  . WiSeBE: Window-Based Sentence Boundary Evaluation 125 F 1 R W  = 2  ×   precision R W  ×  recall R W precision R W  +  recall R W (11) precision R W  =  b j ∈ C T   1 [ b j  = 1 , b j  ∈  w ∀ w  ∈  R W  ]  b j ∈ C T   1 [ b j  = 1] (12) recall R W  =  w k ∈ R W   1 [ w k  ∋  b ∀ b  ∈  C T  ] p (13) Equations  12  and  13  describe precision and recall of  C T  with respect to  R W  . Precision is the number of boundaries  b j  inside any window  w k  from  R W  divided by the total number of boundaries  b j  in  C T  . Recall corresponds to the number of windows  w  with at least one boundary  b  divided by the number of windows w  in  R W  . 4 Evaluating with  W iSeBE To exemplify the  WiSeBE  score we evaluated and compared the performance of two diﬀerent SBD systems over a set of YouTube videos in a multi-reference environment. The ﬁrst system (S1) employs a Convolutional Neural Network to determine if the middle word of a sliding window corresponds to a SU bound- ary or not [ 6 ]. The second approach (S2) by contrast, introduces a bidirectional Recurrent Neural Network model with attention mechanism for boundary detec- tion [ 28 ]. In a ﬁrst glance we performed the evaluation of the systems against each one of the references independently. Then, we implemented a multi-reference evaluation with  WiSeBE . 4.1 Dataset We focused evaluation over a small but diversiﬁed dataset composed by 10 YouTube videos in the English language in the news context. The selected videos cover diﬀerent topics like technology, human rights, terrorism and politics with a length variation between 2 and 10 min. To encourage the diversity of content format we included newscasts, interviews, reports and round tables. During the transcription phase we opted for a manual transcription process because we observed that using transcripts from an ASR system will diﬃcult in a large degree the manual segmentation process. The number of words per transcript oscilate between 271 and 1,602 with a total number of 8,080. We gave clear instructions to three evaluators ( ref 1 , ref 2 , ref 3 ) of how seg- mentation was needed to be perform, including the SU concept and how punctu- ation marks were going to be taken into account. Periods (.), question marks (?), exclamation marks (!) and semicolons (;) were considered SU delimiters (bound- aries) while colons (:) and commas (,) were considered as internal SU marks. The number of segments per transcript and reference can be seen in Table  2 . An interesting remark is that  ref 3  assigns about 43% less boundaries than the mean of the other two references. 126 C.-E. Gonz´alez-Gallardo and J.-M. Torres-Moreno Table 2.  Manual dataset segmentation Reference  v 1 v 2 v 3 v 4 v 5 v 6 v 7 v 8 v 9 v 10  Total ref 1 38 42 17 11 55 87 109 72 55 16 502 ref 2 33 42 16 14 54 98 92 65 51 20 485 ref 3 23 20 10 6 39 39 76 30 29 9 281 4.2 Evaluation We ran both systems (S1 & S2) over the manually transcribed videos obtaining the number of boundaries shown in Table  3 . In general, it can be seen that S1 predicts 27% more segments than S2. This diﬀerence can aﬀect the performance of S1, increasing its probabilities of false positives. Table 3.  Automatic dataset segmentation System  v 1 v 2 v 3 v 4 v 5 v 6 v 7 v 8 v 9 v 10  Total S1 53 38 15 13 54 108 106 70 71 11 539 S2 38 37 12 11 36 92 86 46 53 13 424 Table  4  condenses the performance of both systems evaluated against each one of the references independently. If we focus on F1 scores, performance of both systems varies depending of the reference. For  ref 1 , S1 was better in 5 occasions with respect of S2; S1 was better in 2 occasions only for  ref 2 ; S1 overperformed S2 in 3 occasions concerning  ref 3  and in 4 occasions for  mean  ( bold ). Also from Table  4  we can observe that  ref 1  has a bigger similarity to S1 in 5 occasions compared to other two references, while  ref 2  is more similar to S2 in 7 transcripts (underline). After computing the mean F1 scores over the transcripts, it can be concluded that in average S2 had a better performance segmenting the dataset compared to S1, obtaining a F1 score equal to 0.510. But... What about the complexity of the dataset? Regardless all references have been considered, nor agreement or disagreement between them has been taken into account. All values related to the  WiSeBE  score are displayed in Table  5 . The Agree- ment Ratio ( R G AR ) between references oscillates between 0.525 for  v 8  and 0.767 for  v 5 . The lower the  R G AR , the bigger the penalization  WiSeBE  will give to the ﬁnal score. A good example is S2 for transcript  v 4  where  F 1 R W  reaches a value of 0.800, but after considering  R G AR  the  WiSeBE  score falls to 0.462. It is feasible to think that if all references are taken into account at the same time during evaluation ( F 1 R W  ), the score will be bigger compared to an average of independent evaluations ( F 1 mean ); however this is not always true. That is the case of S1 in  v 10, which present a slight decrease for  F 1 R W  compared to F 1 mean . WiSeBE: Window-Based Sentence Boundary Evaluation 127 Table 4.  Independent multi-reference evaluation Transcript System  ref 1 ref 2 ref 3 Mean P R F1 P R F1 P R F1 P R F1 v 1 S1 0.396 0.553 0.462 0.377 0.606 0.465 0.264 0.609 0.368 0.346 0.589 0.432 S2 0.474 0.474  0.474  0.474 0.545  0.507  0.368 0.6087  0.459  0.439 0.543 0.480 v 2 S1 0.605 0.548  0.575  0.711 0.643  0.675  0.368 0.700 0.483  0.561 0.630 0.578 S2 0.595 0.524 0.557 0.676 0.595 0.633 0.351 0.650 0.456 0.541 0.590 0.549 v 3 S1 0.333 0.294 0.313 0.267 0.250 0.258 0.200 0.300 0.240 0.267 0.281 0.270 S2 0.417 0.294  0.345  0.417 0.313  0.357  0.250 0.300 0.273  0.361 0.302 0.325 v 4 S1 0.615 0.571 0.593 0.462 0.545 0.500 0.308 0.667 0.421 0.462 0.595 0.505 S2 0.909 0.714  0.800  0.818 0.818  0.818  0.455 0.833 0.588  0.727 0.789 0.735 v 5 S1 0.630 0.618  0.624  0.593 0.593  0.593  0.481 0.667 0.560  0.568 0.626 0.592 S2 0.667 0.436 0.527 0.611 0.407 0.489 0.500 0.462 0.480 0.593 0.435 0.499 v 6 S1 0.491 0.541  0.515  0.454 0.563 0.503 0.213 0.590 0.313 0.386 0.565 0.443 S2 0.500 0.469 0.484 0.522 0.552  0.536  0.250 0.590 0.351  0.4234 0.537 0.457 v 7 S1 0.594 0.578  0.586  0.462 0.533 0.495 0.406 0.566 0.473 0.487 0.559 0.518 S2 0.663 0.523 0.585 0.558 0.522  0.539  0.465 0.526 0.494  0.562 0.524 0.539 v 8 S1 0.443 0.477 0.459 0.514 0.500 0.507 0.229 0.533 0.320 0.395 0.503 0.429 S2 0.609 0.431  0.505  0.652 0.417  0.508  0.370 0.567 0.447  0.543 0.471 0.487 v 9 S1 0.437 0.564 0.492 0.451 0.627 0.525 0.254 0.621 0.360 0.380 0.603 0.459 S2 0.623 0.600  0.611  0.585 0.608  0.596  0.321 0.586 0.414  0.509 0.598 0.541 v 10 S1 0.818 0.450  0.581  0.818 0.450  0.581  0.455 0.556 0.500  0.697 0.523 0.582 S2 0.692 0.450 0.545 0.615 0.500 0.552 0.308 0.444 0.364 0.538 0.4645 0.487 Mean scores S1 — 0.520 — 0.510 — 0.404 — 0.481 S2 — 0.543 — 0.554 — 0.433 — 0.510 An important remark is the behavior of S1 and S2 concerning  v 6 . If evalu- ated without considering any (dis)agreement between references ( F 1 mean ), S2 overperforms S1; this is inverted once the systems are evaluated with  WiSeBE . 5 </corps>
	<conclusion>
 Conclusions In this paper we presented WiSeBE, a semi-automatic multi-reference sentence boundary evaluation protocol based on the necessity of having a more reliable way for evaluating the SBD task. We showed how  WiSeBE  is an inclusive metric which not only evaluates the performance of a system against all references, but also takes into account the agreement between them. According to your point of view, this inclusivity is very important given the diﬃculties that are present when working with spoken language and the possible disagreements that a task like SBD could provoke. WiSeBE  shows to be correlated with standard SBD metrics, however we want to measure its correlation with extrinsic evaluations techniques like auto- matic summarization and machine translation. Acknowledgments.  We would like to acknowledge the support of CHIST-ERA for funding this work through the Access Multilingual Information opinionS (AMIS), (France - Europe) project. We also like to acknowledge the support given by the Prof. Hanifa Boucheneb from VERIFORM Laboratory ( ´ Ecole Polytechnique de Montr´eal). </conclusion>
	<discussion>
 Discussion 5.1 R G A R  and Fleiss’ Kappa correlation In Sect.  3  we described the  WiSeBE  score and how it relies on the  R G AR  value to scale the performance of  C T  over  R W  .  R G AR  can intuitively be consider an agreement value over all elements of  R . To test this hypothesis, we computed the Pearson correlation coeﬃcient ( PCC ) [ 21 ] between  R G AR  and the Fleiss’ Kappa [ 4 ] of each video in the dataset ( κ R ). A linear correlation between  R G AR  and  κ R  can be observed in Table  6 . This is conﬁrmed by a  PCC  value equal to 0 . 890, which means a very strong positive linear correlation between them. 5.2 F  1 mean  vs.  W iSeBE Results form Table  5  may give an idea that  WiSeBE  is just an scaled  F 1 mean . While it is true that they show a linear correlation,  WiSeBE  may produce a 128 C.-E. Gonz´alez-Gallardo and J.-M. Torres-Moreno Table 5.  WiSeBE  evaluation Transcript System  F 1 mean  F 1 R W R G AR WiSeBE v 1 S1 0.432 0.495 0.691 0.342 S2 0.480 0.513 0.354 v 2 S1 0.578 0.659 0.688 0.453 S2 0.549 0.595 0.409 v 3 S1 0.270 0.303 0.684 0.207 S2 0.325 0.400 0.274 v 4 S1 0.505 0.593 0.578 0.342 S2 0.735 0.800 0.462 v 5 S1 0.592 0.614 0.767 0.471 S2 0.499 0.500 0.383 v 6 S1 0.443 0.550 0.541 0.298 S2 0.457 0.535 0.289 v 7 S1 0.518 0.592 0.617 0.366 S2 0.539 0.606 0.374 v 8 S1 0.429 0.494 0.525 0.259 S2 0.487 0.508 0.267 v 9 S1 0.459 0.569 0.604 0.344 S2 0.541 0.667 0.403 v 10 S1 0.582 0.581 0.619 0.359 S2 0.487 0.545 0.338 Mean scores S1 0.481 0.545 0.631 0.344 S2 0.510 0.567 0.355 Table 6.  Agreement within dataset Agreement metric v 1 v 2 v 3 v 4 v 5 v 6 v 7 v 8 v 9 v 10 R GAR 0.691 0.688 0.684 0.578 0.767 0.541 0.617 0.525 0.604 0.619 κ R 0.776 0.697 0.757 0.696 0.839 0.630 0.743 0.655 0.704 0.718 diﬀerent system ranking than  F 1 mean  given the integral multi-reference principle it follows. However, what we consider the most proﬁtable about  WiSeBE  is the twofold inclusion of all available references it performs. First, the construction of R W  to provide a more inclusive reference against to whom be evaluated and then, the computation of  R G AR , which scales the result depending of the agreement between references. WiSeBE: Window-Based Sentence Boundary Evaluation 129 6 Conclusions In this paper we presented WiSeBE, a semi-automatic multi-reference sentence boundary evaluation protocol based on the necessity of having a more reliable way for evaluating the SBD task. We showed how  WiSeBE  is an inclusive metric which not only evaluates the performance of a system against all references, but also takes into account the agreement between them. According to your point of view, this inclusivity is very important given the diﬃculties that are present when working with spoken language and the possible disagreements that a task like SBD could provoke. WiSeBE  shows to be correlated with standard SBD metrics, however we want to measure its correlation with extrinsic evaluations techniques like auto- matic summarization and machine translation. Acknowledgments.  We would like to acknowledge the support of CHIST-ERA for funding this work through the Access Multilingual Information opinionS (AMIS), (France - Europe) project. We also like to acknowledge the support given by the Prof. Hanifa Boucheneb from VERIFORM Laboratory ( ´ Ecole Polytechnique de Montr´eal). </discussion>
	<biblio>
 References 1. Bohac, M., Blavka, K., Kucharova, M., Skodova, S.: Post-processing of the recog- nized speech for web presentation of large audio archive. In: 2012 35th International Conference on Telecommunications and Signal Processing (TSP), pp. 441–445. IEEE (2012) 2. Brum, H., Araujo, F., Kepler, F.: Sentiment analysis for Brazilian portuguese over a skewed class corpora. In: Silva, J., Ribeiro, R., Quaresma, P., Adami, A., Branco, A. (eds.) PROPOR 2016. LNCS (LNAI), vol. 9727, pp. 134–138. Springer, Cham (2016).  https://doi.org/10.1007/978-3-319-41552-9 14 3. Che, X., Wang, C., Yang, H., Meinel, C.: Punctuation prediction for unsegmented transcript based on word vector. In: LREC (2016) 4. Fleiss, J.L.: Measuring nominal scale agreement among many raters. Psychol. Bull. 76 (5), 378 (1971) 5. Fohr, D., Mella, O., Illina, I.: New paradigm in speech recognition: deep neural net- works. In: IEEE International Conference on Information Systems and Economic Intelligence (2017) 6. Gonz´alez-Gallardo, C.E., Hajjem, M., SanJuan, E., Torres-Moreno, J.M.: Tran- scripts informativeness study: an approach based on automatic summarization. In: Conf´erence en Recherche d’Information et Applications (CORIA), Rennes, France, May (2018) 7. Gonz´alez-Gallardo, C.E., Torres-Moreno, J.M.: Sentence boundary detection for French with subword-level information vectors and convolutional neural networks. arXiv preprint  arXiv:1802.04559  (2018) 8. Gotoh, Y., Renals, S.: Sentence boundary detection in broadcast speech transcripts. In: ASR2000-Automatic Speech Recognition: Challenges for the new Millenium ISCA Tutorial and Research Workshop (ITRW) (2000) </biblio>
</article>